% !TEX program = xelatex
\documentclass[UTF8]{ctexart}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[english]{datetime2}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{graphicx}   % 插图用的宏包
% 可选：如果你想方便指定默认图片路径，可以加一行：
\graphicspath{{figures/}} % 告诉 LaTeX 图片都在 figures 文件夹里

\usepackage{float}
% 调整页边距，提高空间利用率
\geometry{
    a4paper,
    left=2cm,
    right=2cm,
    top=2.5cm,
    bottom=2.5cm
}

% 章节标题左对齐，格式为 "1  Introduction"（无点号）
\ctexset{
    section/format = \Large\bfseries\raggedright,
    section/aftername = \quad
}

% 页眉页脚设置：清空页眉，页脚显示章节名和页码（字体调小）
\pagestyle{fancy}
\fancyhf{}  % 清空所有页眉页脚
\fancyfoot[L]{\small\leftmark}  % 左下角显示章节名
\fancyfoot[R]{\small\thepage}   % 右下角显示页码
\renewcommand{\headrulewidth}{0pt}  % 去掉页眉横线

\title{Note and Formula of DDA4210}
\author{softstar}
\date{\today}

\begin{document}

\maketitle

\section{Lec1: introduction}
There are no formulas in Lecture 1. Good luck with the rest!

\section{Lec2: Advanced Ensemble Learning}

\subsection{Gradient Boosting:}
Boosting is an ensemble technique that combines multiple weak learners to create a strong learner. The main idea is to train models sequentially, with each model focusing on the errors made by the previous ones.
(用人话说，就是一波接一波地训练模型，每一波都专注于纠正前一波的错误，从而逐步提升整体的预测能力。)

\begin{itemize}
    \item \textbf{Final Model:} \[H_T(\mathbf{x}) = \sum_{t=1}^{T} \alpha_t h_t(\mathbf{x})\]
    
    \begin{itemize}
        \item The weighted sum of all weak learners(加权和).
        \item $H_T(\mathbf{x})$ is the final model after $T$ rounds.
        \item $h_t(\mathbf{x})$ is the $t$-th weak learner.
        \item $\alpha_t$ is the weight of the $t$-th weak learner.
    \end{itemize}
    
    \item \textbf{Loss Function:} \[\mathcal{L}(H) := \frac{1}{n} \sum_{i=1}^{n} \ell(H(\mathbf{x}_i), y_i)\]

    \begin{itemize}
        \item \textbf{Each Step:} We want to add a function $h$ to minimize the loss as fast as possible. Using first-order Taylor expansion(一阶泰勒展开):
        \[
        \mathcal{L}(H + \alpha h) \approx \mathcal{L}(H) + \alpha \langle \nabla \mathcal{L}(H), h \rangle
        \]
        
        \item \textbf{Find $h$:} Minimize $\langle \nabla \mathcal{L}(H), h \rangle$, i.e.:
        \[
        h = \arg\min_{h \in \mathbb{H}} \sum_{i=1}^{n} \frac{\partial \mathcal{L}}{\partial [H(\mathbf{x}_i)]} h(\mathbf{x}_i)
        \]
        
        \begin{itemize}
            \item Here $\frac{\partial \mathcal{L}}{\partial [H(\mathbf{x}_i)]}$ is the gradient of the loss function w.r.t. the current model's output on the $i$-th sample.
            \item We train $h$ to fit these \textbf{negative gradients(负梯度)}. 
        \end{itemize}
       
        
    \end{itemize}
     \item \textbf{GBR with squared error loss: } For regression tasks with squared error loss:
     \[\ell(H(\mathbf{x}_i), y_i) = \sum_{i=1}^n (y_i - H(\mathbf{x}_i))^2\]
        \begin{itemize}
            \item Solve $h_{t+1} = \arg\min_{h \in \mathbb{H}} \sum_{i=1}^{n} q_i h(\mathbf{x}_i)$, where $q_i = \frac{\partial \mathcal{L}}{\partial [H(\mathbf{x}_i)]}$
            
            \item Let $\sum_{i=1}^{n} h^2(\mathbf{x}_i) = \text{constant}$ (we can always normalize the predictions(对结果归一化处理)) and replace $q_i$ with $-2r_i$. We have
            \begin{align}
                h_{t+1} &= \arg\min_{h \in \mathbb{H}} \sum_{i=1}^{n} q_i h(\mathbf{x}_i) \notag \\
                &= \arg\min_{h \in \mathbb{H}} -2 \sum_{i=1}^{n} r_i h(\mathbf{x}_i) \notag \\
                &= \arg\min_{h \in \mathbb{H}} \sum_{i=1}^{n} \left( r_i^2 - 2r_i h(\mathbf{x}_i) + (h(\mathbf{x}_i))^2 \right) \\
                &= \arg\min_{h \in \mathbb{H}} \sum_{i=1}^{n} (h(\mathbf{x}_i) - r_i)^2 \notag
            \end{align}
            
            \item We train $h_{t+1}$ to predict $r_i$, which are from the old model $H_t$.
            
            \item The gradient is:
            \[\frac{\partial \mathcal{L}}{\partial [H(\mathbf{x}_i)]} = -2(y_i - H(\mathbf{x}_i))\]
            
            \item So we fit $h$ to the residuals:
            \[r_i = y_i - H(\mathbf{x}_i)\]
            
            \item Update the model:
            \[H_{t}(\mathbf{x}) = H_{t-1}(\mathbf{x}) + \alpha_t h_t(\mathbf{x})\]
        \end{itemize}
    \item \textbf{GBR with Absolute loss (更鲁棒): } For regression tasks with absolute loss:
        \begin{itemize}
            \item Square loss is easy to deal with mathematically but not robust to outliers.
            \item Absolute loss (more robust to outliers):
            \[\ell(y, \hat{y}) = |y - \hat{y}|\]
        \end{itemize}
        
        \begin{itemize}
            \item The gradient is $q_i = \frac{\partial \mathcal{L}}{\partial H(\mathbf{x}_i)} = -\text{sign}(y_i - H(\mathbf{x}_i))$.
            \item Then fit $h$ on $-q_i$, $i = 1, 2, \ldots, n$. (no longer the residuals, different from using the squared loss)
        \end{itemize}
    \item \textbf{GBR with Huber loss(更更鲁棒): } 
        \begin{itemize}
            \item Huber loss (more robust to outliers):
            \[
            \ell(y, \hat{y}) = \begin{cases}
                \frac{1}{2}(y - \hat{y})^2 & |y - \hat{y}| \leq \delta \\
                \delta(|y - \hat{y}| - \delta/2) & |y - \hat{y}| > \delta
            \end{cases}
            \]
            
            \item The gradient is:
            \[
            \frac{\partial \mathcal{L}}{\partial H(\mathbf{x}_i)} = \begin{cases}
                -(y_i - H(\mathbf{x}_i)) & |y_i - H(\mathbf{x}_i)| \leq \delta \\
                -\delta \, \text{sign}(y_i - H(\mathbf{x}_i)) & |y_i - H(\mathbf{x}_i)| > \delta
            \end{cases}
            \]
        \end{itemize}
    \item \textbf{GBM for classification: } 
        \begin{itemize}
            \item Predict probability of $K$ classes:
            \[p_k(\mathbf{x}) = \frac{\exp(h^{(k)}(\mathbf{x}))}{\sum_{c=1}^{K} \exp(h^{(c)}(\mathbf{x}))} \triangleq \hat{y}^{(k)}, \quad k = 1, 2, \ldots, K\]
            
            \item Loss: $\mathcal{L}(H) = \sum_{i=1}^{n} \ell(\mathbf{y}_i, \hat{\mathbf{y}}_i)$ (e.g. cross-entropy or KL divergence)
            
            \item Initialize $H^{(1)}, H^{(2)}, \ldots, H^{(K)}$, iterate until converge or reach max $T$:
            \begin{enumerate}
                \item Calculate negative gradients for every class:
                \[-g_k(\mathbf{x}_i) = -\frac{\partial \mathcal{L}}{\partial [H^{(k)}(\mathbf{x}_i)]}, \quad i = 1, \ldots, n, \ k = 1, \ldots, K\]
                
                \item Fit $h^{(k)}$ to $-g_k(\mathbf{x}_i)$ (负梯度), $k = 1, 2, \ldots, K$.
                
                \item Update: $H^{(k)} \leftarrow H^{(k)} + \alpha h^{(k)}$, $k = 1, 2, \ldots, K$.
            \end{enumerate}
        \end{itemize}
\end{itemize}

\subsection{AdaBoost}
A special case of gradient boosting with exponential loss.

\begin{itemize}
    \item Exponential loss (learns $\alpha$ adaptively):
    \[\mathcal{L}(H) = \sum_{i=1}^{n} e^{-y_i H(\mathbf{x}_i)}\]
    
    \item Gradient: $q_i = \frac{\partial \mathcal{L}}{\partial H(\mathbf{x}_i)} = -y_i e^{-y_i H(\mathbf{x}_i)}$
    
    \item Let $w_i = \frac{1}{Z} e^{-y_i H(\mathbf{x}_i)}$, where $Z = \sum_{i=1}^{n} e^{-y_i H(\mathbf{x}_i)}$ (constant w.r.t $h$), so $\sum_{i=1}^{n} w_i = 1$.
    
    $w_i$ is the relative contribution of $(\mathbf{x}_i, y_i)$ to the overall loss.
    
    \item Binary classification: $y \in \{-1, +1\}$, $h(\mathbf{x}) \in \{-1, +1\}$.
    
    \item Derivation:
    \begin{align}
        h &= \arg\min_{h \in \mathbb{H}} \sum_{i=1}^{n} q_i h(\mathbf{x}_i) \notag \\
        &= \arg\min_{h \in \mathbb{H}} -\sum_{i=1}^{n} w_i y_i h(\mathbf{x}_i) \notag \\
        &= \arg\min_{h \in \mathbb{H}} \sum_{i: h(\mathbf{x}_i) \neq y_i} w_i - \sum_{i: h(\mathbf{x}_i) = y_i} w_i \notag \\
        &= \arg\min_{h \in \mathbb{H}} \sum_{i: h(\mathbf{x}_i) \neq y_i} w_i \notag
    \end{align}
    * Last equality holds because $\sum_{i=1}^{n} w_i = 1$.
    
    \item Result: 
    \[h = \arg\min_{h \in \mathbb{H}} \sum_{i: h(\mathbf{x}_i) \neq y_i} w_i\]
    
    \[\epsilon := \sum_{i: h(\mathbf{x}_i) \neq y_i} w_i\] is the weighted classification error(加权错误率).
    
    Note: misclassified points by $H$ get larger weights(分类错误的点会得到更大的权重).

    \item Given $h$, find $\alpha$ via:
    \[\alpha = \arg\min_{\alpha} \mathcal{L}(H + \alpha h) = \arg\min_{\alpha} \sum_{i=1}^{n} e^{-y_i(H(\mathbf{x}_i) + \alpha h(\mathbf{x}_i))}\]
    
    \item Differentiate w.r.t $\alpha$ and set to zero:
    \[\sum_{i=1}^{n} y_i h(\mathbf{x}_i) e^{-(y_i H(\mathbf{x}_i) + \alpha y_i h(\mathbf{x}_i))} = 0\]
    
    It follows that:
    \[\sum_{i: h(\mathbf{x}_i) y_i = 1} e^{-(y_i H(\mathbf{x}_i) + \alpha y_i h(\mathbf{x}_i))} - \sum_{i: h(\mathbf{x}_i) y_i = -1} e^{-(y_i H(\mathbf{x}_i) + \alpha y_i h(\mathbf{x}_i))} = 0\]
    \[\sum_{i: h(\mathbf{x}_i) y_i = 1} w_i e^{-\alpha} - \sum_{i: h(\mathbf{x}_i) y_i = -1} w_i e^{\alpha} = 0\]
    
    We have $(1 - \epsilon) e^{-\alpha} - \epsilon e^{\alpha} = 0$, $e^{2\alpha} = \frac{1-\epsilon}{\epsilon}$, and get:
    \[\boxed{\bm{\alpha = \frac{1}{2} \ln \frac{1 - \epsilon}{\epsilon}}}\]
    
\end{itemize}

\subsection{Mixture of Experts(MoE)}
A machine learning technique where multiple expert learners 
(e.g. neural networks) are used to divide a problem space into
homogeneous regions (distinct subtasks).
(用人话(AI)说就是，把一个复杂的问题拆分成多个子任务，每个子任务由一个专家模型来处理，从而提升整体的学习效果。)\\
(骗你的人话也没看懂。。)

    \subsubsection{The First Attempt}
\begin{itemize}
    \item Error function:
    \[E = \left\| y - \sum_{j=1}^{k} g_j O_j \right\|^2\]
    
    \item $y$: target vector; $O_j$: output of expert $j$; $g_j$: proportional contribution of expert $j$.
    
    \item *This error function does not ensure localisation of experts.(人话：专家没有明确的分工).
\end{itemize}

\subsubsection{The Second Attempt}
\begin{itemize}
    \item Error function:
    \[E = \sum_{j=1}^{k} g_j \| y - O_j \|^2\]
    
    \item The system tends to devote a single expert to each training case.
    
    \item *This may not work well in practice.(人话：实际效果可能不佳)
\end{itemize}

\subsubsection{The Third Attempt [Jacobs et al. 1991]}
\begin{itemize}
    \item Error function (mixture of Gaussians):
    \[E_{ME} = -\log \sum_{j=1}^{k} g_j \exp\left( -\frac{1}{2} (y - O_j)^T \Sigma^{-1} (y - O_j) \right)\]
    
    \item Assume $\Sigma = I$ ($\Sigma$: Covariance matrix, 协方差矩阵), derivative w.r.t the $i$-th expert:
    \[\frac{\partial E_{ME}}{\partial O_i} = -\left[ \frac{g_i \exp\left( -\frac{1}{2} (y - O_i)^T (y - O_i) \right)}{\sum_j g_j \exp\left( -\frac{1}{2} (y - O_j)^T (y - O_j) \right)} \right] (y - O_i)\]
    
    \item Compare with derivative of second attempt:
    \[\frac{\partial E}{\partial O_i} = -2g_i (y - O_i)\]
    
    \item The former considers how well expert $i$ performs relative to others, adapting the best-fitting expert faster.
    
    E.g., $g_1 = 0.8$, $g_2 = 0.2$, then $\frac{0.8 \times 0.9}{0.8 \times 0.9 + 0.2 \times 0.1} \approx 0.97 > 0.8$.\\
    (人话：表现好的专家会得到更快的提升)
\end{itemize}
(这nm都是什么玩意?)

\subsection{Stacking(堆叠法)}
Multiple base learners' outputs $(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_N)$ are combined by a meta-learner (stacker) to produce the final prediction $\hat{Y}$.
(用人话说就是，把多个模型的预测结果再拿去训练一个新的模型，从而提升整体的预测效果。)
\begin{itemize}
    \item Multi-level stacking: stacking can be repeated.
    \item Popular stackers: linear models (fast), gradient boosting (accurate).
    \item Base models should be diverse, expert at different data parts.
    \item Trade-off: accurate but slow to predict.
\end{itemize}
(为什么一讲能有这么多神秘小知识点和神秘公式？)

\newpage
\section{Lec3: Advanced Applications}

\subsection{Recommendation Systems}

\subsubsection{Collaborative Filtering(协同过滤)}
Core idea: Use behavioral data from many users (e.g., ratings, clicks) to predict what the current user might like.
(人话：大数据推荐你喜欢的东西)

\begin{itemize}
    \item \textbf{User-Item Interaction:}
    \begin{itemize}
        \item Explicit Feedback(显式反馈): ratings, purchases.
        \item Implicit Feedback(隐式反馈): clicks, browsing time.
    \end{itemize}
    \item \textbf{User-Item Rating Matrix:} Rows = users, columns = items, values = ratings. Typically large and sparse.
    \item \textbf{Distance/Similarity Measurement(相似度度量):}
    \begin{itemize}
        \item Euclidean distance: $\text{sim}(user_i, user_j) = \frac{1}{1 + \|\mathbf{x}_i - \mathbf{x}_j\|_2}$
        \item Cosine similarity: $\text{sim}(user_i, user_j) = \frac{\mathbf{x}_i \cdot \mathbf{x}_j}{\|\mathbf{x}_i\| \|\mathbf{x}_j\|}$
        \item Pearson correlation: $\rho_{X,Y} = \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y}$
    \end{itemize}

    \item \textbf{Nearest-Neighbor Collaborative Filtering(最近邻协同过滤):}
    
    Predict utility of item $i$ based on similar users who rated that item.
    \begin{itemize}
        \item $\mathcal{N}$: neighborhood set (most similar users to $u$ who rated item $i$)(人话：给你推荐东西的那些“邻居”用户)
        \item $w_{uv} \in [0,1]$: similarity weight between users $u$ and $v$(人话：你和邻居用户的相似度)
        \item Prediction:
        \[\hat{x}_{ui} = \bar{x}_u + \sum_{v \in \mathcal{N}} \left( (x_{vi} - \bar{x}_v) \times \frac{w_{uv}}{\sum_{v' \in \mathcal{N}} w_{uv'}} \right)\]
        \item $\bar{x}_u = \frac{1}{|I_u|} \sum_{i \in I_u} x_{ui}$ (average rating of user $u$, where $I_u$ is the set of items rated by $u$)
        \item $\bar{x}_v = \frac{1}{|I_v|} \sum_{i \in I_v} x_{vi}$ (average rating of user $v$, where $I_v$ is the set of items rated by $v$)
    \end{itemize}

    \item \textbf{Matrix Factorization Collaborative Filtering(矩阵分解协同过滤):}
    \begin{itemize}
        \item Notations: 
        \begin{itemize}
            \item $R = [r_{ui}] \in \mathbb{R}^{m \times n}$: incomplete user-item rating matrix
            \item $\Omega$: set of observed entries (known ratings)
            \item $P = [p_1, \ldots, p_u, \ldots, p_m] \in \mathbb{R}^{f \times m}$, \quad $Q = [q_1, \ldots, q_i, \ldots, q_n] \in \mathbb{R}^{f \times n}$
        \end{itemize}
        
        \item Basic SVD ($R \approx P^T Q$):
        \[\min_{P,Q} \sum_{(u,i) \in \Omega} \left\{ (r_{ui} - p_u^T q_i)^2 + \lambda (\|p_u\|^2 + \|q_i\|^2) \right\}\]
        
        \item SVD with bias ($b_{ui} = \mu + b_u + b_i$):
        \[\min_{P,Q,B} \sum_{(u,i) \in \Omega} \left\{ (r_{ui} - \mu - b_u - b_i - p_u^T q_i)^2 + \lambda (\|p_u\|^2 + \|q_i\|^2 + b_u^2 + b_i^2) \right\}\]
        
        \item $\mu$: global mean; $b_u$: user bias; $b_i$: item bias; 
        \item $\lambda(\cdot)$: regularization term (prevents overfitting).
        \item Optimization: GD/SGD or Alternating Least Squares(交替最小二乘法).
    \end{itemize}

    \item \textbf{Pros \& Cons of Collaborative Filtering:}
    \begin{itemize}
        \item Pros: No domain knowledge needed; captures diverse user preferences(人话：不需要领域知识，能捕捉多样的用户偏好).
        \item Cons: Cold-start problem (new users/items); data sparsity(人话：新来的用户和物品数据较少，导致推荐效果差).
    \end{itemize}
\end{itemize}

\subsubsection{Content-Based Methods}
    \begin{itemize}
        \item \textbf{Content analysis:} item $\to$ feature vector $v$ (e.g. TF-IDF, image features).
        \item \textbf{Profile learning:} user $\to$ feature vector $z$ (e.g. age, sex, education).
        \item \textbf{Filtering module:} train classification/regression model to predict user's utility for an item.
        \item \textbf{Recommendation for user:}
        \begin{itemize}
            \item $n$: \# of items; 
            \item $z_u \in \mathbb{R}^d$: 
            \item user $u$'s feature vector; 
            \item $h: \mathbb{R}^d \to \mathbb{R}^n$ (e.g. neural network);
            \item $h_i$: $i$-th output of $h$.
            \item $\ell$: loss function (e.g. squared loss).
            \[\min_h \sum_{(u,i) \in \Omega} \ell(r_{ui}, h_i(z_u))\]
        \end{itemize}

        \item \textbf{Recommendation for item:}
        \begin{itemize}
            \item $m$: \# of users; 
            \item $v_i \in \mathbb{R}^{d'}$: item 
            \item $i$'s feature vector; 
            \item $g: \mathbb{R}^{d'} \to \mathbb{R}^m$ (e.g. neural network); 
            \item $g_u$: $u$-th output of $g$.
            \item $\ell$: loss function (e.g. squared loss).
            \[\min_g \sum_{(u,i) \in \Omega} \ell(r_{ui}, g_u(v_i))\]
        \end{itemize}
        \item \textbf{Pros \& Cons of Content-Based Methods:}
        \begin{itemize}
            \item Pros: User-independent; explainable; handles new items/users well.
            \item Cons: Needs domain knowledge; narrow recommendations (similar items).
        \end{itemize}
    \end{itemize}

\subsubsection{Hybrid Methods(看起来是不重要的知识点)}
Most modern systems are hybrid recommenders.
    \begin{itemize}
        \item Combine separate recommenders (CF + CB): ensemble techniques (linear weighting, stacking, etc.)
        \item Add content-based aspects to CF: e.g. matrix factorization with side information.
    \end{itemize}

\subsubsection{Evaluation Metrics for RS}
   \paragraph{3.1.4.1 Prediction Metrics(评分预测指标)}
    \begin{itemize}
        \item \textbf{Mean Absolute Error (MAE):}
        \[\text{MAE} = \frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}} |r_{ui} - \hat{r}_{ui}|\]
        
        \item \textbf{Root Mean Squared Error (RMSE):}
        \[\text{RMSE} = \sqrt{\frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}} (r_{ui} - \hat{r}_{ui})^2}\]
        
        Here $\mathcal{T}$ denotes the set of user--item pairs used for evaluation (e.g., the test set).(人话：测试集中所有用户和物品的组合)
    \end{itemize}
    \paragraph{3.1.4.2 Ranking-based Metrics(基于排序的指标)}
    \begin{itemize}
        \item \textbf{Precision@K:} fraction of top-$K$ recommended items that are relevant(人话：前K个推荐中有多少是相关的).
        \[\text{Prec}(R)_k = \frac{|\{r \in R:\ r \le k\}|}{k}\]

        \item \textbf{Recall@K:} fraction of relevant items covered in top-$K$(人话：前K个推荐覆盖了多少相关的物品).
        \[\text{Recall}(R)_k = \frac{|\{r \in R:\ r \le k\}|}{|R|}\]
        \begin{itemize}
            \item (Precision = TP/(TP+FP); Recall = TP/(TP+FN)).
            \item $r$ = rank position of a recommended item; 
            \item $k$ = cut-off (top-$k$); 
            \item $R$ = set of relevant items for the user.
        
        \end{itemize}
        \item \textbf{Average Precision (AP@N):} average of precision values at ranks of relevant items.(前N个推荐中相关物品的精确率)
        \[\text{AP@}N = \frac{1}{m} \sum_{k=1}^{N} P(k) \cdot \text{rel}(k)\]
        where $P(k)$ is precision@k, $m$ number of relevant items, and $\text{rel}(k)$ is indicator if item at rank $k$ is relevant.

        \item \textbf{Mean Average Precision (MAP):} mean of AP over $Q$ users: 
        \[\text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}(q)\]

        \item \textbf{Normalized Discounted Cumulative Gain (NDCG):} evaluates ranked relevance with position discounting.
        \[\text{NDCG}_p = \frac{\text{DCG}_p}{\text{IDCG}_p},\quad \text{DCG}_p = \sum_{i=1}^{p} \frac{2^{\text{rel}_i}-1}{\log_2(i+1)}\]
        where $\text{rel}_i$ is relevance of item at rank $i$, and $\text{IDCG}_p$ is the ideal DCG (sorted by relevance). \\
        Range: $[0,1]$. (NDCG越接近1越好)

        \begin{itemize}
            \item \textbf{Example:} 5 recommended items with relevances $[3,2,1,0,2]$ (in rank order).
            \begin{align*}
            \text{DCG}_5 &= \frac{2^{3}-1}{\log_2(1+1)} + \frac{2^{2}-1}{\log_2(2+1)} + \frac{2^{1}-1}{\log_2(3+1)} + \frac{2^{0}-1}{\log_2(4+1)} + \frac{2^{2}-1}{\log_2(5+1)} \approx 10.5538,\\
            \text{IDCG}_5 &= \text{DCG}_5(\text{sorted rel}=[3,2,2,1,0]) \approx 10.8235,\\
            \text{NDCG}_5 &= \dfrac{\text{DCG}_5}{\text{IDCG}_5} \approx 0.975.
            \end{align*}
        \end{itemize}
    (这一坨又是什么玩意？)
    \end{itemize}
\subsection{Learning to Rank(L2R) (排序学习)}
%
\par
Learning to Rank (L2R) trains models to order items by relevance, 
optimizing ranking-specific objectives such as pairwise or listwise losses.
(你说得对，但是这里好像也没有什么公式啊(雾))
\begin{itemize}
    \item L2R is a supervised learning problem for ranking.
    \item Training data consists of:
    \begin{itemize}
        \item A set of queries $Q = \{q_1, \ldots, q_m\}$
        \item A set of documents $D$
        \item For each query $i$, relevant documents $D_i = \{d_{i,1}, \ldots, d_{i,n_i}\} \subseteq D$
        \item Relevance scores $\mathbf{y}_i = (y_{i,1}, \ldots, y_{i,n_i})$ for each $d_{i,j}$
    \end{itemize}
    \item Goal: Given a new query $q$, output a sorted list of relevant documents.
\end{itemize}

\begin{itemize}
    \item \textbf{Point-wise Modeling:} Predicts each (query, document) pair independently.\newline
    $\quad$\textit{Pro:} Simple, can use regression/classification.\newline
    $\quad$\textit{Con:} Ignores relative order between documents.\\
    (一句话：点对点建模简单但忽略了文档间的相对顺序。)
    \item \textbf{Pair-wise Modeling:} Predicts preference between document pairs for a query.\newline
    $\quad$\textit{Pro:} Models relative order.\newline
    $\quad$\textit{Con:} Cannot distinguish excellent-bad from fair-bad pairs.\\
    (一句话：成对建模能捕捉相对顺序，但无法区分优秀-差和一般-差的对比。)
    \item \textbf{List-wise Modeling:} Predicts for the whole ranked list of documents.\newline
    $\quad$\textit{Pro:} Considers position in ranking, aligns with ranking metrics.\newline
    $\quad$\textit{Con:} High training complexity.\\
    (一句话：列表建模考虑排名位置，但训练复杂度高。)
\end{itemize}

\begin{itemize}
    \item \textbf{Evaluation for L2R:} Use benchmark datasets and ranking metrics (e.g., MAP, NDCG).
\end{itemize}

\begin{itemize}
    \item \textbf{Algorithms for L2R:}
    \begin{itemize}
        \item \textbf{Example: Ranking SVM (pairwise):}
        \begin{itemize}
            \item \textbf{Goal:} Learn a scoring function $h(x)=w^\top x$ such that for any pair with labels $y_i>y_j$ we have $h(x_i) > h(x_j)$.
            (人话：让相关性更高的文档得分更高)
            \item \textbf{Training pairs:} Construct pair set $\mathcal{P}=\{(i,j): y_i>y_j\}$; $m=|\mathcal{P}|$ denotes number of pairs.
            \item \textbf{Optimization (primal):}
            \[\begin{aligned}
                &\min_{w,\,\xi_{ij}\ge 0} \ \frac{1}{2}\|w\|^2 + \frac{C}{m} \sum_{(i,j)\in\mathcal{P}} \xi_{ij} \\
                &\text{s.t.} \qquad w^\top x_i \ge w^\top x_j + 1 - \xi_{ij}, \quad \forall (i,j) \in \mathcal{P}.
            \end{aligned}\]
            \item \textbf{Notes:} $\xi_{ij}$ are hinge-loss slacks; $C$ controls margin vs. training error. The objective is equivalent to minimizing the average pairwise hinge loss.
            \item \textbf{Prediction:} Score each document by $h(x)=w^\top x$ and sort descending to produce a ranking.
            \item \textbf{Remarks:} Works well for pairwise preferences; training can be expensive due to $O(n^2)$ pairs, so sampling or stochastic methods are often used. Kernel SVMs and regularization extend naturally.
            \item (人话总结：Ranking SVM通过学习一个线性评分函数来排序文档，优化目标是最大化正确排序对的间隔+最小化排序错误的惩罚。训练时需要处理大量文档对，复杂度高。)
        \end{itemize}
    \end{itemize}
\end{itemize}
(叽里咕噜说什么在)

\section{Lec4-1: Graph Cut and Spectral Clustering(谱聚类)}
\subsection{Graph Paritition}
A similarity graph G=(V,E,W) represents data points as vertices V, 
with an edge in E when the pairwise similarity is positive and weights W storing those affinities.
The affinity matrix records these pairwise similarities.
Graph partitioning (clustering) aims to split the graph so that edges inside a group have large weights while edges across groups have small weights.
(人话：图划分就是把图分成若干部分，使得每个部分内的节点之间联系紧密(组内权重大)，而不同部分之间的联系较弱(组间权重小)。)
\\

Given data points, a similarity graph can be constructed using methods such as \textbf{k-nearest neighbor or $\epsilon$-neighborhood}. The edge weights are often defined by a \textbf{Gaussian kernel}:
\[
k(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
\]

\subsection{Minimum Cut}
\begin{itemize}
    \item Minimum cut: partition the graph into two sets $A$ and $B$ minimizing
    $\mathrm{cut}(A,B):=\sum_{i\in A,\,j\in B} w_{ij}$.
    \item Solvable efficiently (e.g. via max-flow/min-cut algorithms, typical cost O(|V||E|)),
    but the minimum cut often yields unbalanced solutions (it may isolate vertices).
    (人话：最小割可以高效求解，但结果往往不平衡，可能会把一些节点孤立出来。)
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{figures/minimum-cut.png}
        \caption{Minimum Cut Example}
        \label{fig:minimum-cut}
    \end{figure}
    (画的不错的一张图)
    \item To address this, we can use \textbf{Normalized Cut (Ncut)}:
\end{itemize}

\subsection{Normalized Cut}
% Concise summary of Normalized Cut
Normalized Cut balances cut weight with cluster sizes. For a partition (A,B), define
\[\mathrm{vol}(A)=\sum_{i\in A} d_i,\quad d_i=\sum_{j} w_{ij},\]
and
\[\mathrm{Ncut}(A,B):=\mathrm{cut}(A,B)\left(\frac{1}{\mathrm{vol}(A)}+\frac{1}{\mathrm{vol}(B)}\right).
\]
\begin{itemize}
    \item Minimizing Ncut favors balanced partitions but is \textbf{NP-hard};
    \item \textbf{spectral clustering (谱聚类)} provides an efficient relaxation.
\end{itemize}
 
\subsubsection{Degree Matrix and Graph Laplacian}

\[
W=\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1N} \\
w_{21} & w_{22} & \cdots & w_{2N} \\
\vdots & \vdots & \ddots & \vdots \\
w_{N1} & w_{N2} & \cdots & w_{NN}
\end{bmatrix}
\]

\[
D=\begin{bmatrix}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_N
\end{bmatrix},\qquad d_j=\sum_{i=1}^N w_{ij}.
\]

\begin{itemize}
    \item $D$ is the \textbf{degree matrix}. And the \textbf{graph Laplacian matrix} is defined as
    \[L:=D-W.
    \]
    \item Properties: $L$ is symmetric positive semi-definite, $\mathbf{1}^\top L=0$, and its eigenvectors are used in spectral clustering (relaxation of Ncut).
\end{itemize}

\subsubsection{Normalized Cut and Graph Laplacian}

\subsubsection*{Mathematical derivation (optional)}

Recall $L=D-W$ and $D=\mathrm{diag}(d_1,\ldots,d_N)$. 

Let $u=[u_1,u_2,\ldots,u_N]^\top$ with
\[
u_i=\begin{cases}
\dfrac{1}{\mathrm{vol}(A)}, &\text{if } i\in A,\\[4pt]
-\dfrac{1}{\mathrm{vol}(B)}, &\text{if } i\in B.
\end{cases}
\]

Then
\[
u^\top L u=\tfrac{1}{2}\sum_{i,j} w_{ij}(u_i-u_j)^2=\sum_{i\in A, j\in B} w_{ij}\left(\frac{1}{\mathrm{vol}(A)}+\frac{1}{\mathrm{vol}(B)}\right)
\]
and
\[
u^\top D u=\sum_i d_i u_i^2=\sum_{i\in A}\frac{d_i}{\mathrm{vol}(A)^2}+\sum_{j\in B}\frac{d_j}{\mathrm{vol}(B)^2}=\frac{1}{\mathrm{vol}(A)}+\frac{1}{\mathrm{vol}(B)}.
\]

Therefore
\[
\frac{u^\top L u}{u^\top D u}=\sum_{i\in A, j\in B} w_{ij}\left(\frac{1}{\mathrm{vol}(A)}+\frac{1}{\mathrm{vol}(B)}\right)=\mathrm{Ncut}(A,B).
\]
(这几把都是什么玩意？好像是上面的推导吧)\\
(不管了，反正是 optional，摆在这图个吉利)

\subsubsection*{Conclusion}
\begin{itemize}
    \item Ncut is equivalent to minimizing the Rayleigh quotient $\dfrac{u^\top L u}{u^\top D u}$, i.e.,
    \[
    \min_{A,B}\mathrm{Ncut}(A,B)\iff \min_{u}\frac{u^\top L u}{u^\top D u},\quad u\in\mathbb{R}^N,\quad
    u_i=\begin{cases}\dfrac{1}{\mathrm{vol}(A)},&i\in A,\\[4pt]-\dfrac{1}{\mathrm{vol}(B)},&i\in B.\end{cases}
    \]
    \item Equivalent formulation: minimize the quotient subject to $u^\top D\mathbf{1}=0$ and binary constraints $u_i\in\{1,-b\}$ (for some $b>0$).
    \item Relaxation: 
    \[
    L u = \lambda D u,
    \]
    taking the eigenvector corresponding to the \textbf{second smallest eigenvalue} as the relaxed solution.
    \item Equivalently, use the normalized Laplacian $\widetilde{L}=D^{-1}L=I-D^{-1}W$.\\ Obtain a binary partition by thresholding $u$ at $0$: $i\in A$ if $u_i\ge0$, else $i\in B$.
    \item Extend to $k$ clusters by using the first $k$ nontrivial eigenvectors and applying $k$-means (spectral clustering).
    \item (人话：2类划分找第二小特征值对应的特征向量，k类划分用前k个非平凡特征向量，然后k-means聚类。)
\end{itemize}
(事实上还是没看懂，插个眼以后复习的时候看看有没有什么需要补的)

\subsection{Spectral Clustering Algorithm}
\begin{itemize}
    \item Input: data $X=\{x_1,x_2,\ldots,x_N\}$ and number $K$ of clusters.
    \item \textbf{Step 1}: Construct a similarity (affinity) matrix $W$ (e.g. Gaussian kernel)
    \[w_{ij}=\exp\left(-\frac{\|x_i-x_j\|^2}{2\sigma^2}\right)\]
    and build either a $k$-nearest neighbor or $\epsilon$-neighborhood graph.
    \item \textbf{Step 2}: Compute Laplacian $L$ (or normalized variants):
    \[L=D-W,\qquad \widetilde{L}=D^{-1}L=I-D^{-1}W,\qquad \hat{L}_{sym}=I-D^{-1/2}WD^{-1/2}.
    \]
    \item \textbf{Step 3}: Eigen-decompose (normalized) Laplacian and take first $K$ nontrivial eigenvectors to form $Z= [v_1,\ldots,v_K]^\top\in\mathbb{R}^{K\times N}$.(对 L 做特征值分解)
    \item \textbf{Step 4}: Normalize the columns (row-wise embedding) to unit $\ell_2$ norm(归一化):
    \[z_i\leftarrow z_i/\|z_i\|,\quad i=1,\ldots,N.
    \]
    \item \textbf{Step 5}: Run $K$-means on $\{z_1,\ldots,z_N\}$ and output $K$ clusters (assignments on $Z$ or map back to $X$).
    \item Notes: use $\widetilde{L}_{sym}$ (symmetric normalized) for best numerical stability; thresholding the second eigenvector recovers a binary partition.
    \item \textbf{Properties of L:}\\
    For $L = D - W$ or $\hat{L} = I - D^{-1/2} W D^{-1/2}$
    \begin{itemize}
        \item $L$ (and $\hat{L}$) are symmetric and positive semi-definite.(对称 + 半正定)
        \item Eigenvalues satisfy $0=\lambda_1\le\lambda_2\le\cdots\le\lambda_N$.
        \item The multiplicity $K$ of eigenvalue $0$ equals the number of connected components of the graph (hence $K$ clusters).
        (0特征值的数量等于图的连通分量数，也就是聚类数)
    \end{itemize}
\end{itemize}

\section{Lec4-2: Semi-Supervised Learning (SSL, 半监督学习)}
\subsection{Notations}

\begin{itemize}
    \item Input (or feature) $\mathbf{x} \in \mathcal{X}$, output (or label) $\mathbf{y} \in \mathcal{Y}$
    \item Learner $f : \mathcal{X} \to \mathcal{Y}$
    \item Labeled data $(X_l, Y_l) = \{(\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_l, \mathbf{y}_l)\}$
    \item Unlabeled data $X_u = \{\mathbf{x}_{l+1}, \dots, \mathbf{x}_N\}$, available during training
    \item Loss function $\ell : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$
    \item Usually, $l \ll N$
    \item Test data $X_{\text{test}} = \{ \mathbf{x}_{N+1}, \dots \}$, not available during training
\end{itemize}
\paragraph{Why Semi-Supervised Learning?} ~\\
Labeled data is often \textbf{scarce, expensive}, and requires expert annotation or specialized equipment. 
In contrast, unlabeled data is \textbf{abundant and cost-effective}. 
By leveraging both, we can build more robust and high-performing machine learning models.

\subsection{Self-Training Algorithm}
% --- 第一个 subsubsection：自训练算法基础 ---
\paragraph{Assumption:} One's own high confidence predictions are correct.(人话：自己高置信度的预测是正确的)

\subsubsection{Algorithm Procedure:}
\begin{enumerate}
    \item Train model $f$ from labeled data $(X_l, Y_l)$.
    \item Predict labels for unlabeled data $x \in X_u$. (用有标签数据训练模型，然后对无标签数据进行预测)
    \item Add the pseudo-labeled pairs $(x, f(x))$ to the labeled training set. (把预测结果当作新的标签加入训练集)
    \item Repeat the process until a stopping criterion is met.
\end{enumerate}

% --- 第二个 subsubsection：算法变体 ---
\subsubsection{Variations of Self-Training}
Depending on how pseudo-labeled data is incorporated, there are several variations:
\begin{itemize}
    \item \textbf{Most Confident:} Add only a few samples $(x, f(x))$ with the highest prediction confidence to the labeled data. (人话：只添加那些模型最有信心的预测结果)
    \item \textbf{All Samples:} Add all predicted $(x, f(x))$ directly to the labeled data.(直接把所有预测结果都加入训练集)
    \item \textbf{Weighted Approach:} Add all $(x, f(x))$ to the labeled data, but assign different weights to each sample according to its prediction confidence.
    (根据预测置信度给每个样本分配不同的权重)
\end{itemize}

% --- 第三个 subsubsection：具体应用实例 ---
\subsubsection{Self-Training Algorithm: Propagating 1-NN (传播1-NN)}
This is a specific instance of self-training using the 1-Nearest Neighbor (1-NN) approach:
\begin{enumerate}
    \item Classify an unlabeled sample $x$ using the 1-NN rule (assigning the label of its nearest labeled neighbor).(用1-NN方法对无标签样本进行分类，分配最近的有标签邻居的标签)
    \item Add the newly labeled pair $(x, f(x))$ to the labeled dataset and repeat the process.
\end{enumerate}

\paragraph{Advantages and Disadvantages} 
\begin{itemize}
    \item \textbf{Pros:} It is the simplest semi-supervised approach, serves as a versatile wrapper method, and is widely utilized in Natural Language Processing (NLP).
    \item \textbf{Cons:} The method is sensitive to outliers, and early classification errors can lead to self-reinforcing mistakes.
    (对异常值敏感)
\end{itemize}

\subsection{Graph based SSL methods}
\paragraph{Assumption: }A graph is given on the labeled and unlabeled data.  
Instances connected by heavy edge tend to have the same label(由重边连接的实例往往具有相同的标签)
\\
人话就是边的权重反应了两个节点(数据点)，之间的相似程度，边权越大，数据点越相似
\subsubsection{Graph Construction} 
\begin{itemize}
    \item \textbf{Nodes:} The set of nodes is the union of labeled and unlabeled data, $X_l \cup X_u$.
    \item \textbf{Edges:} Constructed via a $k$-Nearest Neighbor graph (unweighted) 
    or a fully connected graph where weights decay with distance (全连接图,权重随距离衰减):
    $$w_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$$
    \item $w_{ij}$ 是高斯核的那个函数，反映了数据点之间的相似度
\end{itemize}

\paragraph{Regularized Classifier}
A standard classifier learns by minimizing a loss term combined with regularization (e.g., LASSO or Regularized Least Squares). We can extend this to semi-supervised learning by using unlabeled data for regularization.

\paragraph{Graph-Based Regularization}
The core principle is that if two instances $x_i$ and $x_j$ are similar (indicated by a large edge weight $w_{ij}$), 
their predicted labels $f(x_i)$ and $f(x_j)$ should also be similar. 
This leads to the following optimization problem: \\
其实就是学习一个学习器来使得损失函数 + 正则化项最小
$$\min_{f} \sum_{i=1}^{l} \ell(y_i, f(x_i)) + \lambda \sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} \|f(x_i) - f(x_j)\|^2$$

\textbf{第一项：} 有标签数据的损失。
\textbf{第二项：} 图正则化项，强制相似点有相似预测。
\textbf{$\lambda$：} 正则化参数。

% Section 7: Consistency and Topology (image_1b3949.png)
\paragraph{Algorithm Objectives}
The graph-based algorithm operates with two primary goals:
\begin{itemize}
    \item \textbf{Label Constraint:} It enforces the correct labels on the existing labeled data.
    \item \textbf{Manifold Consistency:} It maximizes the consistency of unlabeled examples relative to the underlying graph topology.
\end{itemize}

\subsubsection{Label Propagation Algorithm (标签传播算法)}
\begin{enumerate}
    \item Compute affinity matrix $W$ (亲和矩阵).
    \item Compute degree matrix $D$, where $D_{ii} = \sum_j W_{ij}$.
    \item Initialize $\hat{Y}^{(0)} \leftarrow (y_1, \dots, y_l, 0, 0, \dots, 0)$.
    \item Iterate: $\hat{Y}^{(t+1)} = D^{-1}W\hat{Y}^{(t)}$, fix labels of labeled data.
    \item Repeat until convergence.
    \item Assign labels based on the sign of $\hat{y}_i^{(\infty)}$.
\end{enumerate}

\begin{itemize}
    \item The algorithm forces the labels on the labeled data (强制在标记数据上保留标签)
    \item The algorithm tries to maximizes the consistency of the unlabeled examples with the topology of the graph (试图使未标记示例与图的拓扑结构保持最大程度的一致性)
\end{itemize}
其实到最后也不知道这一讲到底细讲了什么，并没有什么公式(雾)

\section{Lec5: GNN}
to be continued...
Happy new year! 
\end{document}